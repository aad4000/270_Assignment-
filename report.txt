              _________ threads____________
size__________ 1        2       4       32   
1,000        |0.0000 seconds | 0.007000 seconds |0.002000 seconds  |0.009000 seconds
1,000,000    | 0.012000 seconds| 0.011000 seconds |0.015000 seconds |0.009000 seconds
1,000,000,000| 3.170000 seconds|2.806000 seconds|2.363000 seconds  |1.403000 seconds

I ran a program to see how fast it counts ones in arrays, changing the array sizes and the number of threads. Here’s what I found:

Small Array (1,000 elements): Surprisingly, using just one thread (doing it the old-school, sequential way) was the quickest. Adding more threads actually made it slower, probably because the computer was spending too much time managing these threads instead of just counting.

Medium Array (1,000,000 elements): Here, things got interesting. Using 32 threads was the best – it was faster than just one or a few threads. Seems like more threads help when there's more data to go through.

Huge Array (1,000,000,000 elements): With this massive array, more threads meant faster counting. The time it took went down significantly from one thread to 32 threads. This shows that for really big tasks, having more threads working together can speed things up a lot.

So, what I learned is that more threads don't always mean faster results – it really depends on how big the job is. For small tasks, keep it simple. But for big data, more threads can really help.

Quick Take on Our Race Condition Experiment

We ran our little experiment to see what happens when threads in a program don't play nice together. Our goal? Count the number ones in a big list of numbers using several threads at the same time, without setting up any rules on how they should take turns.

We did this 100 times, and boy, the results were all over the place! Sometimes we counted way too many ones, sometimes too few. It was a classic example of a race condition - like having a bunch of kids count jellybeans in a jar all at once, and they keep losing track because they're bumping into each other.

The takeaway? If you want accurate results in a program where many threads are doing the same job, you've got to coordinate their work properly. Otherwise, it's chaos, and you can't trust the count.

In tech-speak, we learned the hard way that synchronization is super important. Next time, we'll use some smart programming tricks to make sure every thread waits its turn.

count_mutex :
Using a mutex ensures the accuracy of the counting operation, as it prevents race conditions by securing exclusive access to the shared count variable during updates. This correctness holds true across all thread counts in the experiment.

_nb_of_threads: 2      |4       |8
executing_time:0.008334|0.008901|0.010258

Interestingly, the data shows that as we increase the number of threads, the execution time slightly increases rather than decreases. Typically, more threads can perform more operations concurrently, which can lead to faster execution. However, in this case, since all threads must acquire a lock on the same mutex to update the shared variable, the threads end up waiting for each other. This waiting time becomes more significant with more threads, leading to increased overall execution time.

The increase in execution time with more threads indicates that the overhead of mutex operations is significant compared to the actual work done by each thread. This situation is a classic example of contention in parallel computing, where threads compete for a shared resource, in this case, the mutex lock.

In conclusion, while the mutex is effective in ensuring the correct outcome by preventing a race condition, it introduces contention overhead, which can reduce the benefits of parallel execution, especially with a higher number of threads contending for a single lock

private.c:These private variables are local to each thread and aren't shared, so there's no need for mutex locks to protect the critical section of the code. This approach minimizes the chances of race conditions occurring because each thread independently updates its private variable.

However, it's worth noting that in very rare situations, two or more threads might simultaneously increment the global variable (count) using their private variables. This could potentially lead to a race condition, but such occurrences are extremely uncommon.

The beauty of this approach is that it tends to be faster than using mutex locks. Why? Because there's no waiting time between threads for access to the global variable every time a "1" is found in the array. Each thread operates on its own private variable, reducing contention and improving overall performance.
executing_time: 0.006000 seconds|0.015000 seconds|0.024000 seconds
 As you can see, as we increase the number of threads, the execution time also increases. This is expected because more threads mean more overhead due to thread creation, synchronization, and context switching. However, even with the increase in execution time, using private variables without locks still offers a significant performance advantage over using a mutex, especially when dealing with a small critical section.

 count cache:
               _________ threads____________
size__________ 1                  2               4                   32   
1,000        |0.001000 seconds|0.012000 seconds |0.002000 seconds  |0.014000 seconds
1,000,000    |0.005000 seconds|0.009000 seconds | 0.004000 seconds |0.021000 seconds
1,000,000,000|3.324000 seconds|1.712000 seconds |0.954000 seconds  | 0.668000 seconds


The table shows the execution times for counting ones in arrays of different sizes using 1, 2, 4, and 32 threads. Key observations include:

Small Array (1,000 Elements): Multithreading (2, 4, 32 threads) results in longer execution times compared to a single thread due to the overhead of managing multiple threads, which is disproportionate to the benefits for such a small task.

Medium Array (1,000,000 Elements): A moderate improvement in execution times is observed when using 2 and 4 threads compared to a single thread, indicating the beginning of benefits from parallel processing. However, with 32 threads, the time increases, suggesting an inefficiency likely due to the overhead of managing many threads.

Large Array (1,000,000,000 Elements): Significant performance gains are seen when increasing threads from 1 to 4, showcasing effective parallelization for larger tasks. However, the improvement from 4 to 32 threads is less pronounced, pointing towards diminishing returns due to overhead and potential hardware limitations.

In summary, while multithreading improves performance for larger tasks, the benefits are limited by overhead and the nature of the workload, with diminishing returns as the number of threads increases beyond a certain point.