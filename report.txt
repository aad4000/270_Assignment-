              _________ threads____________
size__________ 1        2       4       32   
1,000        |0.0000 seconds | 0.007000 seconds |0.002000 seconds  |0.009000 seconds
1,000,000    | 0.012000 seconds| 0.011000 seconds |0.015000 seconds |0.009000 seconds
1,000,000,000| 3.170000 seconds|2.806000 seconds|2.363000 seconds  |1.403000 seconds

I ran a program to see how fast it counts ones in arrays, changing the array sizes and the number of threads. Here’s what I found:

Small Array (1,000 elements): Surprisingly, using just one thread (doing it the old-school, sequential way) was the quickest. Adding more threads actually made it slower, probably because the computer was spending too much time managing these threads instead of just counting.

Medium Array (1,000,000 elements): Here, things got interesting. Using 32 threads was the best – it was faster than just one or a few threads. Seems like more threads help when there's more data to go through.

Huge Array (1,000,000,000 elements): With this massive array, more threads meant faster counting. The time it took went down significantly from one thread to 32 threads. This shows that for really big tasks, having more threads working together can speed things up a lot.

So, what I learned is that more threads don't always mean faster results – it really depends on how big the job is. For small tasks, keep it simple. But for big data, more threads can really help.

Quick Take on Our Race Condition Experiment

We ran our little experiment to see what happens when threads in a program don't play nice together. Our goal? Count the number ones in a big list of numbers using several threads at the same time, without setting up any rules on how they should take turns.

We did this 100 times, and boy, the results were all over the place! Sometimes we counted way too many ones, sometimes too few. It was a classic example of a race condition - like having a bunch of kids count jellybeans in a jar all at once, and they keep losing track because they're bumping into each other.

The takeaway? If you want accurate results in a program where many threads are doing the same job, you've got to coordinate their work properly. Otherwise, it's chaos, and you can't trust the count.

In tech-speak, we learned the hard way that synchronization is super important. Next time, we'll use some smart programming tricks to make sure every thread waits its turn.

count_mutex :
Using a mutex ensures the accuracy of the counting operation, as it prevents race conditions by securing exclusive access to the shared count variable during updates. This correctness holds true across all thread counts in the experiment.

_nb_of_threads: 2      |4       |8
executing_time:0.008334|0.008901|0.010258

Interestingly, the data shows that as we increase the number of threads, the execution time slightly increases rather than decreases. Typically, more threads can perform more operations concurrently, which can lead to faster execution. However, in this case, since all threads must acquire a lock on the same mutex to update the shared variable, the threads end up waiting for each other. This waiting time becomes more significant with more threads, leading to increased overall execution time.

The increase in execution time with more threads indicates that the overhead of mutex operations is significant compared to the actual work done by each thread. This situation is a classic example of contention in parallel computing, where threads compete for a shared resource, in this case, the mutex lock.

In conclusion, while the mutex is effective in ensuring the correct outcome by preventing a race condition, it introduces contention overhead, which can reduce the benefits of parallel execution, especially with a higher number of threads contending for a single lock
